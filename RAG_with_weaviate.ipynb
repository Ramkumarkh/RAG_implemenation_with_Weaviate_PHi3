{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2f4981-061b-42fb-b490-a58a788488f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PyPDFLoader from langchain_community.document_loaders module\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Create an instance of PyPDFLoader to handle PDF loading\n",
    "# Specify the path to the PDF file you want to process\n",
    "loader = PyPDFLoader(\"flowers.pdf\")\n",
    "\n",
    "# Load the PDF file and split its content into pages\n",
    "# This function will return a list where each element is the text content of a page\n",
    "pages = loader.load_and_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea2dbdd5-2ac6-4afa-97c1-e2cc69524056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Import RecursiveCharacterTextSplitter from the langchain.text_splitter module\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Import WeaviateVectorStore from the langchain_weaviate.vectorstores module\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "\n",
    "# Import the weaviate module to interact with the Weaviate vector database\n",
    "import weaviate\n",
    "\n",
    "# Create a connection to a locally hosted Weaviate instance\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "# Print the status of the Weaviate client to check if it's ready for operations\n",
    "print(client.is_ready())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efa44218-4a22-41c1-8596-204ee01ae7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Ollama class from the langchain_community.llms module\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "# Define the model name to be used with Ollama\n",
    "model_name = \"phi3\"\n",
    "\n",
    "# Initialize the Ollama model with the specified model name\n",
    "llm = Ollama(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54ae89bc-be65-4713-b7c8-3785f5a61d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the OllamaEmbeddings class from the langchain_community.embeddings module\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Initialize the OllamaEmbeddings with the specified model name\n",
    "# This will generate embeddings using the \"phi3\" model\n",
    "embeddings = OllamaEmbeddings(model=\"phi3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb589f5b-9dca-482b-a729-ef43d5cfdac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import RecursiveCharacterTextSplitter from the langchain.text_splitter module\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Initialize a RecursiveCharacterTextSplitter object\n",
    "# Set the chunk_size to 140 and the chunk_overlap to 40\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Split the pages of the document into chunks using the text splitter\n",
    "# 'pages' should be a list of text documents, and the output will be a list of text chunks\n",
    "splits = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e743e941-2d28-4eda-890f-be1434595a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a WeaviateVectorStore from the provided documents and embeddings\n",
    "# Initialize the Weaviate vector store\n",
    "# - 'splits': List of text chunks obtained from splitting the documents\n",
    "# - 'embeddings': Embedding model to convert text chunks into vectors\n",
    "# - 'client': Weaviate client to interact with the Weaviate instance\n",
    "# - 'index_name': Name of the collection in Weaviate to store the vectors\n",
    "# - 'text_key': The key under which the text will be stored in the Weaviate schema\n",
    "\n",
    "db = WeaviateVectorStore.from_documents(\n",
    "    documents=splits,         # The text chunks to be indexed\n",
    "    embeddings=embeddings,    # The model used to generate embeddings\n",
    "    client=client,            # Weaviate client for database interaction\n",
    "    index_name='MyCollection',# Name of the Weaviate collection\n",
    "    text_key='text'           # Key to access the text content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "092012d2-911a-4bef-8a4b-af2b92cebc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = WeaviateVectorStore(client,index_name='MyCollection',embedding=embeddings,text_key='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ded8b967-7935-4116-ad34-8897cb321fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary classes and functions from langchain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain import hub\n",
    "\n",
    "# Pull a pre-defined prompt from the Langchain hub\n",
    "# The 'rlm/rag-prompt' is a pre-configured prompt used for retrieval-augmented generation (RAG)\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Initialize a RetrievalQA chain using the specified language model and retriever\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,                        # The language model (llm) used for generating answers\n",
    "    retriever=db.as_retriever(),# The retriever method from the WeaviateVectorStore instance\n",
    "    chain_type_kwargs={\"prompt\": prompt} # Additional keyword arguments including the custom prompt\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "319e3ac2-4945-47fd-b362-958059a3d5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\projects\\LegalLLm\\pythonProject\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Bluebells are perennial plants within the Asparagaceae family, indigenous '\n",
      " \"to Atlantic Europe's regions. Unlike other families mentioned such as \"\n",
      " 'Caryophyllaceae or Asteraceae which have different plant types (herbaceous '\n",
      " 'and flowering), bluebells stand out for their specific habitat preference in '\n",
      " 'the Asparagaceae lineage.')\n"
     ]
    }
   ],
   "source": [
    "# Define a question for the QA system\n",
    "question = \"tell me about plants in the family Asparagaceae\"\n",
    "\n",
    "# Use the RetrievalQA chain to process the query and generate an answer\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "# Import the pprint module for pretty-printing the result\n",
    "import pprint\n",
    "\n",
    "# Create a PrettyPrinter object with an indentation of 4 spaces for better readability\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "# Pretty-print the result obtained from the QA chain\n",
    "pp.pprint(result[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
